{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "204fe8c3",
   "metadata": {},
   "source": [
    "# Deep Learning Theoretical Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a08ad26",
   "metadata": {},
   "source": [
    "### General Background for citation\n",
    "\n",
    "Roy et al. (2018) uses 6 common CNN(MobileNet, VGG16, VGG19, ResNet50, InceptionV3 along with CapsuleNet) with various levels of image degradation to test image classification (https://arxiv.org/abs/1807.10108). Their methods of image degradation were (a) Gaussian white noise, (b) Colored Gaussian noise, (c) salt and pepper noise, (d) motion blur, (e) Gaussian blur, (f) Degradation due to JPEG compression (JPEG quality).\n",
    "The note that shallower image classification models (e.g. VGG) are more resilient to image degradation, i.e. that depth decreases robustness. \n",
    "No relevant future research (adversarial attacks being different). \n",
    "\n",
    "Rekha et al. (2020) used CNN to identify aquatic animals with images “fluctuating degrees of luminous intensity and opacity“. (https://www.researchgate.net/profile/Rekha-B-S/publication/338418046_Fish_Detection_and_Classification_Using_Convolutional_Neural_Networks/links/5efc1b1c458515505080fcbd/Fish-Detection-and-Classification-Using-Convolutional-Neural-Networks.pdf?origin=publication_detail)\n",
    "They use a region-proposal-based CNN called Fast R-CNN to first find an item of question and then use a classifier, with a VGG-16 base. They mention a limitation of time, stating that image detection took around 5s, with classification being near instantaneously. \n",
    "\n",
    "(https://arxiv.org/pdf/1604.04004.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5d605",
   "metadata": {},
   "source": [
    "Types of degradation:\n",
    "    \n",
    "- Motion Blur (vertical, horizontal)\n",
    "- Light Noise (ISO, low-light)\n",
    "- Underexposed\n",
    "- Overexposed\n",
    "- JPEG\n",
    "- Poor focus (gaussian blur?)\n",
    "\n",
    "https://stackoverflow.com/questions/14626880/simulation-of-unfocused-image \n",
    "Their solution to mimic unfocused picture: Point-Spread Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94b86e",
   "metadata": {},
   "source": [
    "## Evidence/Sources for Hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef843fce",
   "metadata": {},
   "source": [
    "Should we use seeds?\n",
    "\n",
    "The Kaggle notebook for computer vision has [a couple](https://www.kaggle.com/code/fork/10781907) [exercises](https://www.kaggle.com/code/fork/11989565) [that](https://www.kaggle.com/code/fork/11991328) use seeds for reproducibility—but potentially only for the grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Reproducability\n",
    "def set_seed(seed=31415):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a58bc93",
   "metadata": {},
   "source": [
    "Here are the hyperparameters for the ImageNet and CapsuleNet networks from the Roy et al. (2018) paper: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8fb502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From their GitHub: https://github.com/prasunroy/cnn-on-degraded-images/blob/master/train_deepcnn.py\n",
    "\n",
    "# configurations\n",
    "# -----------------------------------------------------------------------------\n",
    "PROCESS_SEED = None\n",
    "\n",
    "ARCHITECTURE = 'inceptionv3'\n",
    "INCLUDE_TOPL = False\n",
    "WEIGHTS_INIT = 'imagenet'\n",
    "INPUT_TENSOR = None\n",
    "INPUT_DSHAPE = (299, 299, 3)\n",
    "POOLING_TYPE = None\n",
    "NUM_TCLASSES = 10\n",
    "FREEZE_LAYER = 0\n",
    "NEURONS_FC_1 = 1024\n",
    "NEURONS_FC_2 = 1024\n",
    "DROPOUT_FC12 = 0.5\n",
    "FN_OPTIMIZER = optimizers.sgd(lr=0.0001, momentum=0.5)\n",
    "\n",
    "DATASET_ID = 'synthetic_digits'\n",
    "DATA_TRAIN = 'data/{}/imgs_train/'.format(DATASET_ID)\n",
    "DATA_VALID = 'data/{}/imgs_valid/'.format(DATASET_ID)\n",
    "LABEL_MAPS = 'data/{}/labelmap.json'.format(DATASET_ID)\n",
    "SAVE_AUGMT = False\n",
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 100\n",
    "OUTPUT_DIR = 'output/{}/{}/'.format(DATASET_ID, ARCHITECTURE)\n",
    "\n",
    "AUTH_TOKEN = None\n",
    "TELCHAT_ID = None\n",
    "\n",
    "F_SHUTDOWN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f72fab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_125794/1608542796.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mRECON_COEF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mN_ROUTINGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mF_OPTIMIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARN_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'output/{}/{}/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mARCHITECTURE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizers' is not defined"
     ]
    }
   ],
   "source": [
    "# CapsuleNet\n",
    "\n",
    "# configurations\n",
    "# -----------------------------------------------------------------------------\n",
    "PROCESS_SEED = None\n",
    "\n",
    "ARCHITECTURE = 'capsnet'\n",
    "DECODER_NAME = 'decoder'\n",
    "INPUT_DSHAPE = (104, 104, 3)\n",
    "NUM_TCLASSES = 10\n",
    "\n",
    "DATASET_ID = 'synthetic_digits'\n",
    "DATA_TRAIN = 'data/{}/imgs_train/'.format(DATASET_ID)\n",
    "DATA_VALID = 'data/{}/imgs_valid/'.format(DATASET_ID)\n",
    "LABEL_MAPS = 'data/{}/labelmap.json'.format(DATASET_ID)\n",
    "SAMP_TRAIN = 10000\n",
    "SAMP_VALID = 2000\n",
    "SAVE_AUGMT = False\n",
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 100\n",
    "LEARN_RATE = 0.001\n",
    "DECAY_RATE = 0.9\n",
    "RECON_COEF = 0.0005\n",
    "N_ROUTINGS = 3\n",
    "F_OPTIMIZE = optimizers.adam(lr=LEARN_RATE)\n",
    "OUTPUT_DIR = 'output/{}/{}/'.format(DATASET_ID, ARCHITECTURE)\n",
    "\n",
    "AUTH_TOKEN = None\n",
    "TELCHAT_ID = None\n",
    "\n",
    "F_SHUTDOWN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d46e1",
   "metadata": {},
   "source": [
    "## Reasoning for decisions we made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6b802",
   "metadata": {},
   "source": [
    "### Choises for Image Degradation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0988e9",
   "metadata": {},
   "source": [
    " #### To Do: motion blur\n",
    " \n",
    " \n",
    " https://sh-tsang.medium.com/review-blind-image-blur-estimation-via-deep-learning-blur-classification-96963e65d72b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0d00d",
   "metadata": {},
   "source": [
    "One factor in photograph is exposure which controls the brightness of a given picture. Exposure is often seen as the result from 3 different settings: ISO, Aperture, and Shutter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4135ff48",
   "metadata": {},
   "source": [
    "![Try Out](https://miro.medium.com/max/4800/1*e8jZ7b7HLZWKn9hT1eEq-w.webp \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558bb6b1",
   "metadata": {},
   "source": [
    "While we cannot manipulate the amount of light in a source, the other two we can recreate the effect of Shutter length by in- or decreasing the brightness in a picture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae4803",
   "metadata": {},
   "source": [
    "#### Typical image defects\n",
    "\n",
    "Krell and Michaelis (2008) listed in their conference paper several types of digital image degradations and their causes, including:\n",
    "\n",
    "- Blur being caused by defraction limitations or misfocusing of the lens\n",
    "- Noise being caused by optical sensor electronics and image digitization\n",
    "\n",
    "along with vignetting, which is the reduction of brightness towards the corners and edges of an image—that could be an alternative to low-ISO if we wanted it to be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8cb263",
   "metadata": {},
   "source": [
    "#### Light Noise (ISO)\n",
    "\n",
    "Digital cameras use ISO to manipulate the sensitivity to light, with higher ISO values picking up more ambient light sources but at the same time distorting the image by amplifying the darker regions which have a stronger \"graininess\". (https://medium.com/hd-pro/understanding-how-iso-affects-images-7e4ec599575b). We should therefore be able to recreate this by using a colored Gaussian noise filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce5172",
   "metadata": {},
   "source": [
    "## Notes from Meeting:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d343b9",
   "metadata": {},
   "source": [
    " Framework called LIME (interpret networks)\n",
    "Takes images, changes things on the image to see where things go wrong\n",
    "\n",
    "https://towardsdatascience.com/decrypting-your-machine-learning-model-using-lime-5adc035109b5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5875c1d2",
   "metadata": {},
   "source": [
    "https://www.researchgate.net/publication/326430739_A_Low-Light_Image_Enhancement_Method_Based_on_Image_Degradation_Model_and_Pure_Pixel_Ratio_Prior\n",
    "\n",
    "https://www.kaggle.com/code/basu369victor/low-light-image-enhancement-with-cnn\n",
    "Uses batch_size of 32\n",
    "\n",
    "Find Examples for hyper-parameter\n",
    "Reasoning for decisions (motion blur)\n",
    "\n",
    "https://stackoverflow.com/questions/14626880/simulation-of-unfocused-image \n",
    "Their solution to mimic unfocused picture: Point-Spread Function\n",
    "\n",
    "http://www2.ujf-grenoble.fr/medecine/iab/clientzone/plforme9/fichiers/DeconvolutionMicroscopy_Sibarita_Springer.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ed40c3",
   "metadata": {},
   "source": [
    "## Misc\n",
    "\n",
    "Feynman AI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
